{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnxruntime 1.19.2\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, onnx, onnxruntime as ort\n",
    "from pathlib import Path\n",
    "\n",
    "print('onnxruntime', ort.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 ---------------- Configuration ----------------\n",
    "MODEL_PATH = Path('./RealTimeAlignment/onnx_no-residual/submodule_onnx/submodule_embed.onnx')   # update as necessary\n",
    "DUMP_DIR   = Path('embed_dumps')\n",
    "DUMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Optional: external input file (.npy or .txt); leave None to use random data\n",
    "INPUT_FILE = None\n",
    "BATCH_SIZE = 1\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No INPUT_FILE provided \u2013 generating random input\n",
      "Input shape: (1, 6)\n"
     ]
    }
   ],
   "source": [
    "# Prepare input\n",
    "def load_input():\n",
    "    if INPUT_FILE is None:\n",
    "        print('[INFO] No INPUT_FILE provided \u2013 generating random input')\n",
    "        x = np.random.randn(BATCH_SIZE, 6).astype(np.float16)\n",
    "    else:\n",
    "        if INPUT_FILE.suffix == '.npy':\n",
    "            x = np.load(INPUT_FILE).astype(np.float16)\n",
    "        else:\n",
    "            x = np.loadtxt(INPUT_FILE, dtype=np.float16)\n",
    "        x = x.reshape(BATCH_SIZE, 6).astype(np.float16)\n",
    "    np.savetxt(DUMP_DIR/'dense1_input.txt', x.astype(np.float32).flatten(), fmt='%.6f')\n",
    "    return x\n",
    "\n",
    "x_in = load_input()\n",
    "print('Input shape:', x_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped 1.bias  shape=(128,)\n",
      "Dumped 4.bias  shape=(128,)\n",
      "Dumped onnx::MatMul_13  shape=(6, 128)\n",
      "Dumped onnx::MatMul_14  shape=(128, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1210996/1062612749.py:7: DeprecationWarning: `mapping.TENSOR_TYPE_TO_NP_TYPE` is now deprecated and will be removed in a future release.To silence this warning, please use `helper.tensor_dtype_to_np_dtype` instead.\n",
      "  dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[tensor.data_type]\n"
     ]
    }
   ],
   "source": [
    "# Dump weights/biases\n",
    "model = onnx.load(str(MODEL_PATH))\n",
    "initializer_map = {init.name: init for init in model.graph.initializer}\n",
    "\n",
    "def tensor_to_numpy(tensor):\n",
    "    return np.frombuffer(tensor.raw_data,\n",
    "                         dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[tensor.data_type]\n",
    "                         ).reshape(tensor.dims)\n",
    "\n",
    "def dump_tensor(name, arr):\n",
    "    np.savetxt(DUMP_DIR/f'{name}.txt', arr.astype(np.float32).flatten(), fmt='%.6f')\n",
    "\n",
    "for name, tensor in initializer_map.items():\n",
    "    arr = tensor_to_numpy(tensor)\n",
    "    dump_tensor(f'weights_{name}', arr)\n",
    "    print(f'Dumped {name}  shape={arr.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with intermediate tensors\n",
    "sess_opts = ort.SessionOptions()\n",
    "sess_opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "session = ort.InferenceSession(str(MODEL_PATH), sess_opts, providers=['CPUExecutionProvider'])\n",
    "\n",
    "wanted_ops = ('Gemm', 'LeakyRelu')\n",
    "output_names = []\n",
    "for node in model.graph.node:\n",
    "    if node.op_type in wanted_ops:\n",
    "        output_names.extend(node.output)\n",
    "output_names.append(session.get_outputs()[0].name)\n",
    "# remove duplicates while preserving order\n",
    "seen = set()\n",
    "unique_names = []\n",
    "for n in output_names:\n",
    "    if n not in seen:\n",
    "        unique_names.append(n)\n",
    "        seen.add(n)\n",
    "input_name = session.get_inputs()[0].name\n",
    "results = session.run(unique_names, {input_name: x_in})\n",
    "dense1_out, relu1_out, dense2_out, relu2_out = results\n",
    "dump_tensor('dense1_output', dense1_out)\n",
    "dump_tensor('relu1_input', dense1_out)\n",
    "dump_tensor('relu1_output', relu1_out)\n",
    "dump_tensor('dense2_input', relu1_out)\n",
    "dump_tensor('dense2_output', dense2_out)\n",
    "dump_tensor('relu2_input', dense2_out)\n",
    "dump_tensor('relu2_output', relu2_out)\n",
    "dump_tensor('final_output', relu2_out)\n",
    "print('Dumped intermediate and final outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}