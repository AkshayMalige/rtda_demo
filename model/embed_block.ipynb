{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnxruntime 1.19.2\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, onnx, onnxruntime as ort\n",
    "from pathlib import Path\n",
    "\n",
    "print('onnxruntime', ort.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ ---------------- Configuration ----------------\n",
    "MODEL_PATH = Path('./RealTimeAlignment/onnx_no-residual/submodule_onnx/submodule_embed.onnx')   # update as necessary\n",
    "DUMP_DIR   = Path('embed_dumps')\n",
    "DUMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Optional: external input file (.npy or .txt); leave None to use random data\n",
    "INPUT_FILE = None\n",
    "BATCH_SIZE = 1\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No INPUT_FILE provided â€“ generating random input\n",
      "Input shape: (1, 6)\n"
     ]
    }
   ],
   "source": [
    "# Prepare input\n",
    "def load_input():\n",
    "    if INPUT_FILE is None:\n",
    "        print('[INFO] No INPUT_FILE provided â€“ generating random input')\n",
    "        x = np.random.randn(BATCH_SIZE, 6).astype(np.float32)\n",
    "    else:\n",
    "        if INPUT_FILE.suffix == '.npy':\n",
    "            x = np.load(INPUT_FILE).astype(np.float32)\n",
    "        else:\n",
    "            x = np.loadtxt(INPUT_FILE, dtype=np.float32)\n",
    "        x = x.reshape(BATCH_SIZE, 6).astype(np.float32)\n",
    "    np.savetxt(DUMP_DIR/'input_dense1.txt', x.flatten(), fmt='%.6f')\n",
    "    return x\n",
    "\n",
    "x_in = load_input()\n",
    "print('Input shape:', x_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped 1.bias  shape=(128,)\n",
      "Dumped 4.bias  shape=(128,)\n",
      "Dumped onnx::MatMul_13  shape=(6, 128)\n",
      "Dumped onnx::MatMul_14  shape=(128, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1210996/1062612749.py:7: DeprecationWarning: `mapping.TENSOR_TYPE_TO_NP_TYPE` is now deprecated and will be removed in a future release.To silence this warning, please use `helper.tensor_dtype_to_np_dtype` instead.\n",
      "  dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[tensor.data_type]\n"
     ]
    }
   ],
   "source": [
    "# Dump weights/biases\n",
    "model = onnx.load(str(MODEL_PATH))\n",
    "initializer_map = {init.name: init for init in model.graph.initializer}\n",
    "\n",
    "def tensor_to_numpy(tensor):\n",
    "    return np.frombuffer(tensor.raw_data,\n",
    "                         dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[tensor.data_type]\n",
    "                         ).reshape(tensor.dims)\n",
    "\n",
    "def dump_tensor(name, arr):\n",
    "    np.savetxt(DUMP_DIR/f'{name}.txt', arr.flatten(), fmt='%.6f')\n",
    "\n",
    "for name, tensor in initializer_map.items():\n",
    "    arr = tensor_to_numpy(tensor)\n",
    "    dump_tensor(f'weights_{name}', arr)\n",
    "    print(f'Dumped {name}  shape={arr.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(float)) , expected: (tensor(float16))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         intermediate\u001b[38;5;241m.\u001b[39mextend(node\u001b[38;5;241m.\u001b[39moutput)\n\u001b[1;32m     12\u001b[0m input_name \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m---> 13\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m{\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# for name, arr in zip(intermediate + [session.get_outputs()[0].name], results):\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     dump_tensor(name, arr)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     print(f'Dumped {name}  shape={arr.shape}')\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/hls_env/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(float)) , expected: (tensor(float16))"
     ]
    }
   ],
   "source": [
    "# Inference with intermediate tensors\n",
    "sess_opts = ort.SessionOptions()\n",
    "sess_opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "session = ort.InferenceSession(str(MODEL_PATH), sess_opts, providers=['CPUExecutionProvider'])\n",
    "\n",
    "wanted_ops = {'Gemm', 'LeakyRelu'}\n",
    "intermediate = []\n",
    "for node in model.graph.node:\n",
    "    if node.op_type in wanted_ops:\n",
    "        intermediate.extend(node.output)\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "results = session.run(intermediate + [session.get_outputs()[0].name],\n",
    "                      {input_name: x_in})\n",
    "\n",
    "# for name, arr in zip(intermediate + [session.get_outputs()[0].name], results):\n",
    "#     dump_tensor(name, arr)\n",
    "#     print(f'Dumped {name}  shape={arr.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
