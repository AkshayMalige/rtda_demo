{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0e37452-9d32-411a-a111-92e106c52701"
   },
   "source": [
    "# ONNX Submodule Dumper\n",
    "\n",
    "This notebook provides a complete script to run ONNX submodules and dump the input data, output data, weights, biases, and Leaky ReLU activation outputs to text files for a single input.\n",
    "\n",
    "**Please ensure the following directory structure and files exist before running this notebook:**\n",
    "\n",
    "```\n",
    ".\n",
    "├── checkpoints/\n",
    "│   └── config.yaml\n",
    "├── data/\n",
    "│   └── rom_det-3_part-200_cont-and-rounded_excerpt/\n",
    "│       └── ...\n",
    "├── submodule_onnx/\n",
    "│   ├── submodule_embed.onnx\n",
    "│   ├── submodule_solvers-0.onnx\n",
    "│   ├── submodule_solvers-1.onnx\n",
    "│   ├── submodule_solvers-2.onnx\n",
    "│   └── submodule_output.onnx\n",
    "├── mlp.py\n",
    "└── dump_all_submodules.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b9f0901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== IMPORTS ========================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from rich import print as rprint\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "from rtal.datasets.dataset import ROMDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mlp import MLP\n",
    "from onnx import numpy_helper  # add this import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4839087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- config ----\n",
    "SUBMODULE_DIR = \"submodule_onnx\"\n",
    "EMBED_ONNX    = f\"{SUBMODULE_DIR}/submodule_embed.onnx\"\n",
    "OUTPUT_ONNX   = f\"{SUBMODULE_DIR}/submodule_output.onnx\"\n",
    "\n",
    "NUM_SOLVERS   = 3     # <-- set to your count\n",
    "SUBSET_SIZE   = 6     # <-- set to your assemble_np subset size\n",
    "BATCH_SIZE    = 1\n",
    "NUM_PARTICLES = 50    # <-- match your training/config\n",
    "DATA_ROOT     = \"data/rom_det-3_part-200_cont-and-rounded_excerpt/\"\n",
    "SPLIT         = \"train\"\n",
    "\n",
    "OUT_DIR       = \"onnx_txt\"   # where txt files go\n",
    "SAVE_DTYPE = \"float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165897d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readout shape (torch): (1, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "# ---- dataset → readout ----\n",
    "dataset    = ROMDataset(DATA_ROOT, split=SPLIT, num_particles=NUM_PARTICLES)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "event      = next(iter(dataloader))\n",
    "\n",
    "# readout: (B, num_detectors, num_particles, 2)\n",
    "readout = event['readout_curr_cont']  # torch tensor (CPU)\n",
    "\n",
    "# -> (B, num_particles, num_detectors, 2)\n",
    "readout = torch.transpose(readout, 1, 2)\n",
    "# -> (B, num_particles, num_detectors*2) == (B, num_particles, in_features)\n",
    "readout = readout.flatten(-2, -1)\n",
    "\n",
    "print(\"readout shape (torch):\", tuple(readout.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5c9d402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx_inputs shape: (1, 50, 6) dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# ---- infer dtype from embed ONNX input ----\n",
    "embed_sess = ort.InferenceSession(EMBED_ONNX)\n",
    "embed_in_type = embed_sess.get_inputs()[0].type  # e.g. 'tensor(float16)' or 'tensor(float)'\n",
    "np_dtype = np.float16 if 'float16' in embed_in_type else np.float32\n",
    "\n",
    "onnx_inputs = readout.numpy().astype(np_dtype)\n",
    "print(\"onnx_inputs shape:\", onnx_inputs.shape, \"dtype:\", onnx_inputs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3652c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def assemble_np(array: np.ndarray, subset_size: int) -> np.ndarray:\n",
    "    \"\"\"Roll along axis=1 and concat along the last dim.\"\"\"\n",
    "    return np.concatenate([np.roll(array, shift=i, axis=1) for i in range(subset_size)], axis=-1)\n",
    "\n",
    "def _cast_for_save(arr: np.ndarray) -> np.ndarray:\n",
    "    if SAVE_DTYPE == \"float32\":\n",
    "        return arr.astype(np.float32, copy=False)\n",
    "    elif SAVE_DTYPE == \"float16\":\n",
    "        return arr.astype(np.float16, copy=False)\n",
    "    else:\n",
    "        raise ValueError(f\"SAVE_DTYPE must be 'float16' or 'float32', got {SAVE_DTYPE!r}\")\n",
    "\n",
    "def save_txt(path: Path, arr: np.ndarray):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    arr_to_save = _cast_for_save(arr)     # <-- cast only for saving\n",
    "    np.savetxt(path, np.asarray(arr_to_save).reshape(-1), fmt=\"%.6f\")\n",
    "\n",
    "def run_and_dump(session: ort.InferenceSession, x: np.ndarray, tag: str):\n",
    "    subdir = Path(OUT_DIR) / tag\n",
    "    # save the input (casted per SAVE_DTYPE), but feed the original x to the model\n",
    "    save_txt(subdir / \"input.txt\", x)\n",
    "\n",
    "    in_name  = session.get_inputs()[0].name\n",
    "    out_name = session.get_outputs()[0].name\n",
    "    y = session.run([out_name], {in_name: x})[0]\n",
    "\n",
    "    # save the output (casted per SAVE_DTYPE)\n",
    "    save_txt(subdir / \"output.txt\", y)\n",
    "    return y  # return original dtype for downstream solvers\n",
    "\n",
    "def dump_dense_weights(model_path: str, tag: str):\n",
    "    \"\"\"\n",
    "    Save weights/biases for Linear/Dense layers in an ONNX:\n",
    "      - Gemm:     W = input[1], B = input[2] (if initializer)\n",
    "      - MatMul:   W = right input (if initializer); try to find Add bias after it\n",
    "    Files go under OUT_DIR/<tag>/ as NN_linear_W.txt / NN_linear_B.txt.\n",
    "    NOTE: Saved exactly as stored in ONNX (no transpose handling).\n",
    "    \"\"\"\n",
    "    m = onnx.load(model_path)\n",
    "    inits = {i.name: numpy_helper.to_array(i) for i in m.graph.initializer}\n",
    "\n",
    "    # Build simple consumer map to detect MatMul -> Add bias\n",
    "    consumers = {}\n",
    "    for n in m.graph.node:\n",
    "        for i in n.input:\n",
    "            consumers.setdefault(i, []).append(n)\n",
    "\n",
    "    subdir = Path(OUT_DIR) / tag\n",
    "    subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    idx = 0\n",
    "    for n in m.graph.node:\n",
    "        if n.op_type == \"Gemm\":\n",
    "            W = inits.get(n.input[1]) if len(n.input) > 1 else None\n",
    "            B = inits.get(n.input[2]) if len(n.input) > 2 else None\n",
    "            if W is not None: save_txt(subdir / f\"{idx:02d}_linear_W.txt\", W)\n",
    "            if B is not None: save_txt(subdir / f\"{idx:02d}_linear_B.txt\", B)\n",
    "            idx += 1\n",
    "\n",
    "        elif n.op_type == \"MatMul\" and len(n.input) > 1 and n.input[1] in inits:\n",
    "            # Right input is constant weights\n",
    "            W = inits[n.input[1]]\n",
    "            save_txt(subdir / f\"{idx:02d}_linear_W.txt\", W)\n",
    "\n",
    "            # Look for immediate Add with a constant bias\n",
    "            B = None\n",
    "            for c in consumers.get(n.output[0], []):\n",
    "                if c.op_type == \"Add\":\n",
    "                    other = [t for t in c.input if t != n.output[0]]\n",
    "                    if other and other[0] in inits:\n",
    "                        B = inits[other[0]]\n",
    "                        break\n",
    "            if B is not None:\n",
    "                save_txt(subdir / f\"{idx:02d}_linear_B.txt\", B)\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cbece5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Wrote TXT dumps under: onnx_txt\n"
     ]
    }
   ],
   "source": [
    "# ---- load sessions ----\n",
    "solver_paths = [f\"{SUBMODULE_DIR}/submodule_solvers-{i}.onnx\" for i in range(NUM_SOLVERS)]\n",
    "solver_sess  = [ort.InferenceSession(p) for p in solver_paths]\n",
    "output_sess  = ort.InferenceSession(OUTPUT_ONNX)\n",
    "\n",
    "# ---- run ----\n",
    "# 1) embed\n",
    "embed_out = run_and_dump(embed_sess, onnx_inputs, tag=\"embed\")\n",
    "\n",
    "# 2) solvers\n",
    "arr = embed_out\n",
    "for i, sess in enumerate(solver_sess):\n",
    "    arr = assemble_np(arr, SUBSET_SIZE)\n",
    "    arr = run_and_dump(sess, arr, tag=f\"solvers-{i}\")\n",
    "\n",
    "# 3) output\n",
    "out = run_and_dump(output_sess, arr, tag=\"output\")\n",
    "\n",
    "print(\"Done. Wrote TXT dumps under:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "179598bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights for embed / solvers / output\n",
    "dump_dense_weights(EMBED_ONNX, \"embed\")\n",
    "for i, p in enumerate(solver_paths):\n",
    "    dump_dense_weights(p, f\"solvers-{i}\")\n",
    "dump_dense_weights(OUTPUT_ONNX, \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577af07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Inputs\n",
    "# x      = np.loadtxt(\"input_x.txt\", dtype=np.float32)           # 128\n",
    "# W      = np.loadtxt(\"weights.txt\", dtype=np.float32)           # 16384 (=128*128), row-major\n",
    "# bias   = np.loadtxt(\"bias.txt\",    dtype=np.float32)           # 128\n",
    "\n",
    "# # Reshape weights row-major (rows=outputs, cols=K)\n",
    "# W = W.reshape(128, 128)\n",
    "\n",
    "# # Split along K=128 → two 64-wide blocks\n",
    "# W0 = W[:, :64].reshape(-1)\n",
    "# W1 = W[:, 64:].reshape(-1)\n",
    "\n",
    "# # Split x consistently\n",
    "# x0 = x[:64]\n",
    "# x1 = x[64:]\n",
    "\n",
    "# np.savetxt(\"weights_part0.txt\", W0, fmt=\"%.6f\")\n",
    "# np.savetxt(\"weights_part1.txt\", W1, fmt=\"%.6f\")\n",
    "# np.savetxt(\"x_part0.txt\", x0, fmt=\"%.6f\")\n",
    "# np.savetxt(\"x_part1.txt\", x1, fmt=\"%.6f\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
