{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== IMPORTS ========================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from rich import print as rprint\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "from rtal.datasets.dataset import ROMDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mlp import MLP\n",
    "from onnx import numpy_helper  # add this import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- config ----\n",
    "SUBMODULE_DIR = \"submodule_onnx\"\n",
    "EMBED_ONNX    = f\"{SUBMODULE_DIR}/submodule_embed.onnx\"\n",
    "OUTPUT_ONNX   = f\"{SUBMODULE_DIR}/submodule_output.onnx\"\n",
    "\n",
    "NUM_SOLVERS   = 3     # <-- set to your count\n",
    "SUBSET_SIZE   = 6     # <-- set to your assemble_np subset size\n",
    "BATCH_SIZE    = 1\n",
    "NUM_PARTICLES = 50    # <-- match your training/config\n",
    "DATA_ROOT     = \"data/rom_det-3_part-200_cont-and-rounded_excerpt/\"\n",
    "SPLIT         = \"train\"\n",
    "\n",
    "OUT_DIR       = \"onnx_txt\"   # where txt files go\n",
    "SAVE_DTYPE = \"float32\"\n",
    "TP_CASC_LEN_LAYER2 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- dataset \u2192 readout ----\n",
    "dataset    = ROMDataset(DATA_ROOT, split=SPLIT, num_particles=NUM_PARTICLES)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "event      = next(iter(dataloader))\n",
    "\n",
    "# readout: (B, num_detectors, num_particles, 2)\n",
    "readout = event['readout_curr_cont']  # torch tensor (CPU)\n",
    "\n",
    "# -> (B, num_particles, num_detectors, 2)\n",
    "readout = torch.transpose(readout, 1, 2)\n",
    "# -> (B, num_particles, num_detectors*2) == (B, num_particles, in_features)\n",
    "readout = readout.flatten(-2, -1)\n",
    "\n",
    "print(\"readout shape (torch):\", tuple(readout.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- infer dtype from embed ONNX input ----\n",
    "embed_sess = ort.InferenceSession(EMBED_ONNX)\n",
    "embed_in_type = embed_sess.get_inputs()[0].type  # e.g. 'tensor(float16)' or 'tensor(float)'\n",
    "np_dtype = np.float16 if 'float16' in embed_in_type else np.float32\n",
    "\n",
    "onnx_inputs = readout.numpy().astype(np_dtype)\n",
    "# Pad to 8 features for dense8x128 input\n",
    "if onnx_inputs.shape[1] < 8:\n",
    "    pad_width = ((0,0),(0,8 - onnx_inputs.shape[1]))\n",
    "    onnx_inputs = np.pad(onnx_inputs, pad_width, mode='constant')\n",
    "print(\"onnx_inputs shape:\", onnx_inputs.shape, \"dtype:\", onnx_inputs.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def assemble_np(array: np.ndarray, subset_size: int) -> np.ndarray:\n",
    "    \"\"\"Roll along axis=1 and concat along the last dim.\"\"\"\n",
    "    return np.concatenate([np.roll(array, shift=i, axis=1) for i in range(subset_size)], axis=-1)\n",
    "\n",
    "def _cast_for_save(arr: np.ndarray) -> np.ndarray:\n",
    "    if SAVE_DTYPE == \"float32\":\n",
    "        return arr.astype(np.float32, copy=False)\n",
    "    elif SAVE_DTYPE == \"float16\":\n",
    "        return arr.astype(np.float16, copy=False)\n",
    "    else:\n",
    "        raise ValueError(f\"SAVE_DTYPE must be 'float16' or 'float32', got {SAVE_DTYPE!r}\")\n",
    "\n",
    "def save_txt(path: Path, arr: np.ndarray):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    arr_to_save = _cast_for_save(arr)     # <-- cast only for saving\n",
    "    np.savetxt(path, np.asarray(arr_to_save).reshape(-1), fmt=\"%.6f\")\n",
    "\n",
    "def run_and_dump(session: ort.InferenceSession, x: np.ndarray, tag: str):\n",
    "    subdir = Path(OUT_DIR) / tag\n",
    "    # save the input (casted per SAVE_DTYPE), but feed the original x to the model\n",
    "    save_txt(subdir / \"input.txt\", x)\n",
    "\n",
    "    in_name  = session.get_inputs()[0].name\n",
    "    out_name = session.get_outputs()[0].name\n",
    "    y = session.run([out_name], {in_name: x})[0]\n",
    "\n",
    "    # save the output (casted per SAVE_DTYPE)\n",
    "    save_txt(subdir / \"output.txt\", y)\n",
    "    return y  # return original dtype for downstream solvers\n",
    "\n",
    "def split_dense_weights(W, cascade_len=1):\n",
    "    assert W.shape[0] % cascade_len == 0\n",
    "    split_size = W.shape[0] // cascade_len\n",
    "    return [W[i*split_size:(i+1)*split_size, :] for i in range(cascade_len)]\n\n",
    "def dump_dense_weights(model_path: str, tag: str, cascade_len: int = 1):\n",
    "    \"\"\"\n",
    "    Save weights/biases for Linear/Dense layers in an ONNX:\n",
    "      - Gemm:     W = input[1], B = input[2] (if initializer)\n",
    "      - MatMul:   W = right input (if initializer); try to find Add bias after it\n",
    "    Files go under OUT_DIR/<tag>/ as NN_linear_W.txt / NN_linear_B.txt.\n",
    "    NOTE: Transposed to column-major and optionally split for cascade.\n",
    "    \"\"\"\n",
    "    m = onnx.load(model_path)\n",
    "    inits = {i.name: numpy_helper.to_array(i) for i in m.graph.initializer}\n",
    "\n",
    "    # Build simple consumer map to detect MatMul -> Add bias\n",
    "    consumers = {}\n",
    "    for n in m.graph.node:\n",
    "        for i in n.input:\n",
    "            consumers.setdefault(i, []).append(n)\n",
    "\n",
    "    subdir = Path(OUT_DIR) / tag\n",
    "    subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    idx = 0\n",
    "    for n in m.graph.node:\n",
    "        if n.op_type == \"Gemm\":\n",
    "            W = inits.get(n.input[1]) if len(n.input) > 1 else None\n",
    "            B = inits.get(n.input[2]) if len(n.input) > 2 else None\n",
    "            if W is not None:\n",
    "                W = W.T\n",
    "                if tag == \"embed\" and W.shape[1] < 8:\n",
    "                    W = np.pad(W, ((0,0),(0,8 - W.shape[1])), constant_values=0)\n",
    "                if cascade_len > 1:\n",
    "                    parts = split_dense_weights(W, cascade_len)\n",
    "                    for i, part in enumerate(parts):\n",
    "                        save_txt(subdir / f\"{idx:02d}_linear_W_part{i}.txt\", part)\n",
    "                else:\n",
    "                    save_txt(subdir / f\"{idx:02d}_linear_W.txt\", W)\n",
    "            if B is not None: save_txt(subdir / f\"{idx:02d}_linear_B.txt\", B)\n",
    "            idx += 1\n",
    "\n",
    "        elif n.op_type == \"MatMul\" and len(n.input) > 1 and n.input[1] in inits:\n",
    "            # Right input is constant weights\n",
    "            W = inits[n.input[1]]\n",
    "            W = W.T\n",
    "            if tag == \"embed\" and W.shape[1] < 8:\n",
    "                W = np.pad(W, ((0,0),(0,8 - W.shape[1])), constant_values=0)\n",
    "            if cascade_len > 1:\n",
    "                parts = split_dense_weights(W, cascade_len)\n",
    "                for i, part in enumerate(parts):\n",
    "                    save_txt(subdir / f\"{idx:02d}_linear_W_part{i}.txt\", part)\n",
    "            else:\n",
    "                save_txt(subdir / f\"{idx:02d}_linear_W.txt\", W)\n",
    "\n",
    "            # Look for immediate Add with a constant bias\n",
    "            B = None\n",
    "            for c in consumers.get(n.output[0], []):\n",
    "                if c.op_type == \"Add\":\n",
    "                    other = [t for t in c.input if t != n.output[0]]\n",
    "                    if other and other[0] in inits:\n",
    "                        B = inits[other[0]]\n",
    "                        break\n",
    "            if B is not None:\n",
    "                save_txt(subdir / f\"{idx:02d}_linear_B.txt\", B)\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- load sessions ----\n",
    "solver_paths = [f\"{SUBMODULE_DIR}/submodule_solvers-{i}.onnx\" for i in range(NUM_SOLVERS)]\n",
    "solver_sess  = [ort.InferenceSession(p) for p in solver_paths]\n",
    "output_sess  = ort.InferenceSession(OUTPUT_ONNX)\n",
    "\n",
    "# ---- run ----\n",
    "# 1) embed\n",
    "embed_out = run_and_dump(embed_sess, onnx_inputs, tag=\"embed\")\n",
    "\n",
    "# 2) solvers\n",
    "arr = embed_out\n",
    "for i, sess in enumerate(solver_sess):\n",
    "    arr = assemble_np(arr, SUBSET_SIZE)\n",
    "    arr = run_and_dump(sess, arr, tag=f\"solvers-{i}\")\n",
    "\n",
    "# 3) output\n",
    "out = run_and_dump(output_sess, arr, tag=\"output\")\n",
    "\n",
    "print(\"Done. Wrote TXT dumps under:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights for embed / solvers / output\n",
    "dump_dense_weights(EMBED_ONNX, \"embed\")\n",
    "for i, p in enumerate(solver_paths):\n",
    "    dump_dense_weights(p, f\"solvers-{i}\")\n",
    "dump_dense_weights(OUTPUT_ONNX, \"output\", cascade_len=TP_CASC_LEN_LAYER2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}