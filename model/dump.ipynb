{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# === CONFIG ===\n",
    "ONNX_DIR      = Path(\"submodule_onnx\")\n",
    "EMBED_ONNX    = ONNX_DIR / \"submodule_embed.onnx\"\n",
    "SOLVER_GLOB   = \"submodule_solvers-*.onnx\"\n",
    "OUTPUT_ONNX   = ONNX_DIR / \"submodule_output.onnx\"\n",
    "\n",
    "OUTPUT_ROOT   = Path(\"onnx_txt\")\n",
    "TMP_MODEL_DIR = Path(\"tmp_instrumented_models\")\n",
    "TXT_FLOAT_FMT = \"%.8g\"\n",
    "\n",
    "# ROMDataset\n",
    "DATA_ROOT   = \"data/rom_det-3_part-200_cont-and-rounded_excerpt/\"\n",
    "SPLIT       = \"train\"\n",
    "BATCH_SIZE  = 1\n",
    "FEATURE_KEY = \"readout_curr_cont\"\n",
    "DEVICE = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import onnx, onnxruntime as ort\n",
    "from onnx import numpy_helper, helper, shape_inference\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from rtal.datasets.dataset import ROMDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def sanitize(name: str) -> str:\n",
    "    for b in ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|', ' ']:\n",
    "        name = name.replace(b, '_')\n",
    "    return name\n",
    "\n",
    "def dump_txt(path: Path, arr: np.ndarray, fmt: str = \"%.8g\"):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    flat = np.asarray(arr).ravel()\n",
    "    with open(path, \"w\") as f:\n",
    "        for v in flat:\n",
    "            f.write((fmt % float(v)) + \"\\n\")\n",
    "\n",
    "def ort_session(path: Path):\n",
    "    so = ort.SessionOptions()\n",
    "    so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "    return ort.InferenceSession(str(path), so, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def concrete_dims(shape: Tuple) -> Tuple:\n",
    "    return tuple(int(s) if isinstance(s,(int,np.integer)) else None for s in shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Bias-aware graph introspection ----\n",
    "def build_maps(model):\n",
    "    g = model.graph\n",
    "    init_map = {init.name: numpy_helper.to_array(init) for init in g.initializer}\n",
    "    const_map = {}\n",
    "    producers = {}\n",
    "    for n in g.node:\n",
    "        for o in n.output:\n",
    "            producers[o] = n\n",
    "        if n.op_type == \"Constant\":\n",
    "            for a in n.attribute:\n",
    "                if a.name == \"value\":\n",
    "                    const_map[n.output[0]] = numpy_helper.to_array(a.t)\n",
    "    consumers = {}\n",
    "    for n in g.node:\n",
    "        for i in n.input:\n",
    "            consumers.setdefault(i, []).append(n)\n",
    "    return init_map, const_map, consumers, producers\n",
    "\n",
    "def resolve_bias_tensor(name: Optional[str], init_map, const_map, producers):\n",
    "    if not name:\n",
    "        return None\n",
    "    if name in init_map:\n",
    "        return init_map[name]\n",
    "    if name in const_map:\n",
    "        return const_map[name]\n",
    "    prod = producers.get(name)\n",
    "    if prod is not None and prod.op_type == \"Constant\":\n",
    "        for a in prod.attribute:\n",
    "            if a.name == \"value\":\n",
    "                return numpy_helper.to_array(a.t)\n",
    "    return None\n",
    "\n",
    "def list_dense_with_bias_and_outputs(model) -> List[Dict]:\n",
    "    \"\"\"Identify dense-like operations and their output tensors, weights, and bias.\n",
    "       - Gemm: W=input[1], B=input[2] (resolve Constant as well), out = node.output[0]\n",
    "       - MatMul(+Add): W=MatMul input[1], B=Add's non-MM input (resolve Constant), out = Add output (or MatMul output if no Add)\n",
    "    \"\"\"\n",
    "    dense = []\n",
    "    g = model.graph\n",
    "    init_map, const_map, consumers, producers = build_maps(model)\n",
    "    idx = 0\n",
    "    for n in g.node:\n",
    "        if n.op_type == \"Gemm\":\n",
    "            Wname = n.input[1] if len(n.input)>1 else None\n",
    "            Bname = n.input[2] if len(n.input)>2 else None\n",
    "            W = init_map.get(Wname)\n",
    "            B = resolve_bias_tensor(Bname, init_map, const_map, producers)\n",
    "            dense.append({\"kind\":\"Gemm\",\"index\":idx,\"out\":n.output[0],\"W\":W,\"B\":B})\n",
    "            idx += 1\n",
    "        elif n.op_type == \"MatMul\":\n",
    "            mm_out = n.output[0]\n",
    "            add_node = None\n",
    "            for c in consumers.get(mm_out, []):\n",
    "                if c.op_type == \"Add\":\n",
    "                    add_node = c; break\n",
    "            Wname = n.input[1] if len(n.input)>1 else None\n",
    "            W = init_map.get(Wname)\n",
    "            out = add_node.output[0] if add_node is not None else mm_out\n",
    "            B = None\n",
    "            if add_node is not None:\n",
    "                a0, a1 = add_node.input[0], add_node.input[1]\n",
    "                cand = a1 if a0 == mm_out else a0\n",
    "                B = resolve_bias_tensor(cand, init_map, const_map, producers)\n",
    "            dense.append({\"kind\":\"MatMulAdd\" if add_node else \"MatMul\",\"index\":idx,\"out\":out,\"W\":W,\"B\":B})\n",
    "            idx += 1\n",
    "    return dense\n",
    "\n",
    "def list_lrelu_nodes(model)->List[Dict]:\n",
    "    outs=[]; idx=0\n",
    "    for n in model.graph.node:\n",
    "        if n.op_type==\"LeakyRelu\":\n",
    "            alpha=0.01\n",
    "            for a in n.attribute:\n",
    "                if a.name==\"alpha\": alpha=a.f; break\n",
    "            outs.append({\"index\":idx,\"out\":n.output[0],\"alpha\":float(alpha)}); idx+=1\n",
    "    return outs\n",
    "\n",
    "def instrument_model_for_outputs(model, names: List[str]):\n",
    "    # Add requested value infos as graph outputs, using shape inference where possible\n",
    "    try:\n",
    "        inferred = shape_inference.infer_shapes(model, strict_mode=True)\n",
    "        inferred_vis = list(inferred.graph.input)+list(inferred.graph.output)+list(inferred.graph.value_info)\n",
    "        vi_map = {vi.name: vi for vi in inferred_vis}\n",
    "    except Exception:\n",
    "        vi_map = {}\n",
    "    g = model.graph\n",
    "    existing_outs = {o.name for o in g.output}\n",
    "    to_add = []\n",
    "    for name in names:\n",
    "        if name in existing_outs: \n",
    "            continue\n",
    "        vi = vi_map.get(name, helper.make_tensor_value_info(name, onnx.TensorProto.FLOAT, None))\n",
    "        to_add.append(vi)\n",
    "    g.output.extend(to_add)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Shape adapter & stage runner ----\n",
    "def adapt_for_model(x: np.ndarray, model_in_shape: Tuple, prefer_3d_last: Optional[int]=None) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    dims = concrete_dims(model_in_shape)\n",
    "    want_3d = len(dims)==3 and (dims[0] in (1,None)) and (dims[1] in (50,None))\n",
    "    want_2d = len(dims)==2 and (dims[0] in (1,None))\n",
    "    # Prefer exact (1,50,D)\n",
    "    if want_3d and prefer_3d_last is not None:\n",
    "        D = prefer_3d_last\n",
    "        if x.shape == (1,50,D): return x\n",
    "        if x.ndim==2 and x.shape[1]==50*D: return x.reshape(1,50,D)\n",
    "        if x.ndim==3 and x.shape[1]==50 and x.shape[2]*50 == 50*D: return x[:, :, :D]\n",
    "    # Fit declared dims if concrete\n",
    "    if want_3d and dims[2] is not None:\n",
    "        D = dims[2]\n",
    "        if x.shape == (1,50,D): return x\n",
    "        if x.ndim==2 and x.shape[1]==50*D: return x.reshape(1,50,D)\n",
    "    if want_2d and dims[1] is not None:\n",
    "        D = dims[1]\n",
    "        if x.ndim==2 and x.shape[1]==D: return x\n",
    "        if x.ndim==3 and x.shape[1]*x.shape[2]==D: return x.reshape(1,D)\n",
    "    # Fallback: flatten then trim/pad\n",
    "    flat = x.reshape(1,-1)\n",
    "    if want_2d and dims[1] is not None:\n",
    "        D = dims[1]\n",
    "        if flat.shape[1] >= D: return flat[:, :D]\n",
    "        pad = np.zeros((1,D), dtype=np.float32); pad[:, :flat.shape[1]] = flat; return pad\n",
    "    if want_3d and dims[2] is not None:\n",
    "        D = 50*dims[2]\n",
    "        if flat.shape[1] >= D: return flat[:, :D].reshape(1,50,dims[2])\n",
    "        pad = np.zeros((1,D), dtype=np.float32); pad[:, :flat.shape[1]] = flat; return pad.reshape(1,50,dims[2])\n",
    "    return x\n",
    "\n",
    "\n",
    "def run_stage(model_path, x_in, tag, prefer_3d_last=None):\n",
    "    \"\"\"\n",
    "    Load an ONNX stage, expose Dense/LeakyReLU intermediates as extra graph outputs,\n",
    "    and run inference while dumping:\n",
    "      - input.txt\n",
    "      - dense_{i}_weights.txt, dense_{i}_bias.txt, dense_{i}_output.txt\n",
    "      - leakyrelu_{i}_output.txt, leakyrelu_{i}_alpha.txt\n",
    "      - model_output.txt  (true ORIGINAL graph output; not an appended intermediate)\n",
    "      - graph_outputs.txt (list of true graph output tensor names)\n",
    "\n",
    "    RETURNS:\n",
    "      A numpy array with the stage's TRUE model output (for chaining).\n",
    "      If the graph has multiple outputs, returns the first one.\n",
    "    \"\"\"\n",
    "    import onnx, numpy as np\n",
    "    from pathlib import Path\n",
    "\n",
    "    model_path = Path(model_path)\n",
    "    out_dir = (OUTPUT_ROOT / tag)\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    # --- Load and introspect the graph ---\n",
    "    model = onnx.load(str(model_path))\n",
    "\n",
    "    # Collect dense-like ops and LeakyReLUs (uses your existing helpers)\n",
    "    denses = list_dense_with_bias_and_outputs(model)   # each: {\"kind\",\"index\",\"out\",\"W\",\"B\"}\n",
    "    lrs    = list_lrelu_nodes(model)                   # each: {\"index\",\"out\",\"alpha\"}\n",
    "\n",
    "    # Names we want ORT to return as intermediates (in this order):\n",
    "    names_to_fetch = [d[\"out\"] for d in denses] + [lr[\"out\"] for lr in lrs]\n",
    "\n",
    "    # --- Preserve ORIGINAL graph outputs BEFORE instrumentation ---\n",
    "    orig_out_names = [o.name for o in model.graph.output]\n",
    "\n",
    "    # --- Instrument the model to add intermediate outputs (does NOT change orig_out_names) ---\n",
    "    inst = onnx.ModelProto(); inst.CopyFrom(model)\n",
    "    inst = instrument_model_for_outputs(inst, names_to_fetch)\n",
    "\n",
    "    TMP_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    tmp_model = TMP_MODEL_DIR / f\"{model_path.stem}__inst.onnx\"\n",
    "    onnx.save(inst, str(tmp_model))\n",
    "\n",
    "    # --- Run with ORT (optimizations disabled in ort_session()) ---\n",
    "    sess = ort_session(tmp_model)\n",
    "    in_vi = sess.get_inputs()[0]\n",
    "\n",
    "    # Shape adapt the provided input to match model expectation\n",
    "    xin = adapt_for_model(x_in, tuple(in_vi.shape), prefer_3d_last=prefer_3d_last)\n",
    "\n",
    "    # Dump the exact input tensor we feed\n",
    "    dump_txt(out_dir / \"input.txt\", xin.reshape(-1), fmt=TXT_FLOAT_FMT)\n",
    "\n",
    "    # Build fetch list: intermediates first (in the order we added), then ORIGINAL graph outputs\n",
    "    fetches = names_to_fetch + orig_out_names\n",
    "    outputs = sess.run(fetches, {in_vi.name: xin})\n",
    "\n",
    "    # Split back into dicts\n",
    "    inter_arrs = outputs[:len(names_to_fetch)]\n",
    "    graph_arrs = outputs[len(names_to_fetch):]\n",
    "\n",
    "    name_to_arr = dict(zip(names_to_fetch, inter_arrs))\n",
    "    orig_name_to_arr = dict(zip(orig_out_names, graph_arrs))\n",
    "\n",
    "    # --- Dump Dense intermediates (weights, bias, and outputs) ---\n",
    "    for i, d in enumerate(denses):\n",
    "        W = np.asarray(d[\"W\"])\n",
    "        B = np.asarray(d[\"B\"]) if d[\"B\"] is not None else None\n",
    "        Y = np.asarray(name_to_arr[d[\"out\"]])\n",
    "        dump_txt(out_dir / f\"dense_{i}_weights.txt\", W.reshape(-1), fmt=TXT_FLOAT_FMT)\n",
    "        if B is not None:\n",
    "            dump_txt(out_dir / f\"dense_{i}_bias.txt\",    B.reshape(-1), fmt=TXT_FLOAT_FMT)\n",
    "        dump_txt(out_dir / f\"dense_{i}_output.txt\",  Y.reshape(-1), fmt=TXT_FLOAT_FMT)\n",
    "\n",
    "    # --- Dump LeakyReLU intermediates (outputs and alphas) ---\n",
    "    for i, lr in enumerate(lrs):\n",
    "        Y = np.asarray(name_to_arr[lr[\"out\"]])\n",
    "        dump_txt(out_dir / f\"leakyrelu_{i}_output.txt\", Y.reshape(-1), fmt=TXT_FLOAT_FMT)\n",
    "        dump_txt(out_dir / f\"leakyrelu_{i}_alpha.txt\",  np.array([lr[\"alpha\"]]), fmt=TXT_FLOAT_FMT)\n",
    "\n",
    "    # --- Dump true graph outputs and select return value ---\n",
    "    with open(out_dir / \"graph_outputs.txt\", \"w\") as fh:\n",
    "        for gname in orig_out_names:\n",
    "            fh.write(f\"{gname}\\n\")\n",
    "\n",
    "    if len(orig_out_names) >= 1:\n",
    "        # Canonical \"model_output.txt\" is the FIRST true graph output\n",
    "        true_out = np.asarray(orig_name_to_arr[orig_out_names[0]])\n",
    "        dump_txt(out_dir / \"model_output.txt\", true_out.reshape(-1), fmt=TXT_FLOAT_FMT)\n",
    "    else:\n",
    "        raise RuntimeError(\"Model has no graph outputs; cannot determine model_output.\")\n",
    "\n",
    "    # --- Optional safety: if final node is LeakyReLU and equals the graph output, sanity-check ---\n",
    "    if lrs and len(orig_out_names) >= 1:\n",
    "        last_lr_out = lrs[-1][\"out\"]\n",
    "        if orig_out_names[0] == last_lr_out:\n",
    "            a = np.asarray(name_to_arr[last_lr_out]).reshape(-1)\n",
    "            b = np.asarray(true_out).reshape(-1)\n",
    "            # Should usually be exactly equal; allow tiny fp wiggles\n",
    "            if not np.allclose(a, b, rtol=1e-6, atol=1e-7):\n",
    "                maxdiff = float(np.max(np.abs(a - b)))\n",
    "                print(f\"[WARN] Final LeakyReLU output and model graph output differ (max abs diff={maxdiff}).\")\n",
    "\n",
    "    print(f\"[{tag}] OK: dumped to {out_dir}\")\n",
    "    # Return the TRUE model output (np.ndarray) for chaining\n",
    "    return true_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- ROMDataset and discovery ----\n",
    "def rom_input_b506() -> np.ndarray:\n",
    "    ds = ROMDataset(DATA_ROOT, split=SPLIT, num_particles=50)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    event = next(iter(dl))\n",
    "    x = event[FEATURE_KEY].to(DEVICE)\n",
    "    # Normalize to (1,50,6); common arrival: (1,3,50,2)\n",
    "    if x.ndim == 4 and x.shape[0]==1 and x.shape[1:]==(3,50,2):\n",
    "        x = torch.transpose(x,1,2).reshape(1,50,6)\n",
    "    elif x.shape == (1,50,6):\n",
    "        pass\n",
    "    elif x.ndim >= 3 and (x.shape[-1]*x.shape[-2] == 6):\n",
    "        x = x.reshape(1,50,6)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected ROM shape {tuple(x.shape)}, cannot get (1,50,6)\")\n",
    "    return x.cpu().numpy().astype(np.float32)\n",
    "\n",
    "def discover_solvers() -> List[Path]:\n",
    "    paths = sorted(ONNX_DIR.glob(SOLVER_GLOB), key=lambda p: int(re.search(r\"(\\d+)\", p.stem).group(1)) if re.search(r\"(\\d+)\", p.stem) else 1e9)\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No solver ONNX files found under {ONNX_DIR} with pattern {SOLVER_GLOB}\")\n",
    "    print(\"[DISCOVER] solvers:\", [p.name for p in paths])\n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DISCOVER] solvers: ['submodule_solvers-0.onnx', 'submodule_solvers-1.onnx', 'submodule_solvers-2.onnx']\n",
      "[EMBED] OK: dumped to onnx_txt/EMBED\n",
      "[SOLVER-0] prefers last=768 from ['batch_size', 50, 768]\n",
      "[SOLVER-0] OK: dumped to onnx_txt/SOLVER-0\n",
      "[SOLVER-1] prefers last=768 from ['batch_size', 50, 768]\n",
      "[SOLVER-1] OK: dumped to onnx_txt/SOLVER-1\n",
      "[SOLVER-2] prefers last=768 from ['batch_size', 50, 768]\n",
      "[SOLVER-2] OK: dumped to onnx_txt/SOLVER-2\n",
      "[OUTPUT] OK: dumped to onnx_txt/OUTPUT\n",
      "Chain complete. Dumps are under: onnx_txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Chain runner ----\n",
    "def run_chain():\n",
    "    ensure_dir(OUTPUT_ROOT); ensure_dir(TMP_MODEL_DIR)\n",
    "    if not EMBED_ONNX.exists(): raise FileNotFoundError(f\"embed ONNX not found: {EMBED_ONNX}\")\n",
    "    if not OUTPUT_ONNX.exists(): raise FileNotFoundError(f\"output ONNX not found: {OUTPUT_ONNX}\")\n",
    "    solvers = discover_solvers()\n",
    "    # 1) ROM → (1,50,6)\n",
    "    x = rom_input_b506()\n",
    "    # 2) embed (prefer last=6) → (1,50,768)\n",
    "    x = run_stage(EMBED_ONNX, x, tag=\"EMBED\", prefer_3d_last=6)\n",
    "    # 3) each solver in order — derive preferred last dim from input shape when possible\n",
    "    for i, spath in enumerate(solvers):\n",
    "        sess = ort_session(spath); in_shape = sess.get_inputs()[0].shape\n",
    "        dims = concrete_dims(in_shape)\n",
    "        pref = dims[2] if (len(dims)==3 and isinstance(dims[2], int)) else (dims[1]//50 if len(dims)==2 and isinstance(dims[1], int) and dims[1]%50==0 else None)\n",
    "        print(f\"[SOLVER-{i}] prefers last={pref} from {in_shape}\")\n",
    "        x = run_stage(spath, x, tag=f\"SOLVER-{i}\", prefer_3d_last=pref)\n",
    "    # 4) output (prefer last=128)\n",
    "    x = run_stage(OUTPUT_ONNX, x, tag=\"OUTPUT\", prefer_3d_last=128)\n",
    "    print(\"Chain complete. Dumps are under:\", OUTPUT_ROOT)\n",
    "\n",
    "# Execute\n",
    "run_chain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatMul ['input', 'onnx::MatMul_13'] -> ['/1/MatMul_output_0']\n",
      "Add ['1.bias', '/1/MatMul_output_0'] -> ['/1/Add_output_0']\n",
      "LeakyRelu ['/1/Add_output_0'] -> ['/2/LeakyRelu_output_0']\n",
      "MatMul ['/2/LeakyRelu_output_0', 'onnx::MatMul_14'] -> ['/4/MatMul_output_0']\n",
      "Add ['4.bias', '/4/MatMul_output_0'] -> ['/4/Add_output_0']\n",
      "LeakyRelu ['/4/Add_output_0'] -> ['output']\n",
      "Graph outputs: ['output']\n"
     ]
    }
   ],
   "source": [
    "# m = onnx.load(\"submodule_embed.onnx\")\n",
    "m = onnx.load(EMBED_ONNX)\n",
    "for n in m.graph.node[-6:]:\n",
    "    print(n.op_type, list(n.input), \"->\", list(n.output))\n",
    "print(\"Graph outputs:\", [o.name for o in m.graph.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (6400,) (6400,)\n",
      "max abs diff: 0.0\n",
      "allclose? True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "stage = \"EMBED\"  # change if needed\n",
    "root = Path(\"onnx_txt\")/stage\n",
    "\n",
    "a = np.loadtxt(root/\"leakyrelu_1_output.txt\")\n",
    "b = np.loadtxt(root/\"model_output.txt\")\n",
    "\n",
    "print(\"shapes:\", a.shape, b.shape)\n",
    "diff = np.max(np.abs(a - b))\n",
    "print(\"max abs diff:\", diff)\n",
    "print(\"allclose?\", np.allclose(a, b, rtol=1e-6, atol=1e-7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
