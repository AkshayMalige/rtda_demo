# AI Engine-ML Graph: Four-Layer Neural Network with Roll-Concat

This directory implements a four-layer neural network graph featuring a roll-concat operation and shared-buffer tiling for efficient memory access across a large cascaded matrix multiply.

## Architecture

### Pipeline
1. **`roll_concat`** – Repeats input 128-element vector 6 times to create 768 elements
2. **`dense0`** (768×128, cascade length=12) – Large matrix-vector multiply with shared buffer input
3. **`bias_add`** → **`leaky_relu`** → **`window_split_128_to_64x2`**
4. **`dense1`** (128×128, cascade length=2) – Second dense layer
5. **`bias_add`** → **`leaky_relu`** → **`window_split_128_to_64x2`**
6. **`dense2`** (128×128, cascade length=2) – Third dense layer
7. **`bias_add`** → **`leaky_relu`** → **`window_split_128_to_64x2`**
8. **`dense3`** (128×128, cascade length=2) – Fourth dense layer
9. **`bias_add`** → **`leaky_relu`** – Final output

### Key Features
- **Shared Buffer**: 768-element buffer tiled across 12 cascade kernels for dense0
- **External Weight Buffer**: Off-tile storage that stages the aggregated weight stream before individual cascades consume their slices
- **Roll-Concat**: Efficiently replicates 128→768 elements for large matrix input
- **4 Dense Layers**: Progressive transformation from 768 inputs to 128 outputs
- Weights and biases delivered via PLIO streams into memory buffers (no RTP)

### Layer Configurations
- **Layer 0**: 768×128 dense layer with 12-way cascade (TP_CASC_LEN_LAYER3=12)
- **Layers 1-3**: 128×128 dense layers with 2-way cascade (TP_CASC_LEN_LAYER2=2)
- **Activation**: Leaky ReLU with slope 0.1
- **Tiling**: `ROLL_CONCAT_TILE_SPAN = 768 / 12 = 64` elements per cascade tile

## Directory Layout

```
├── graph.cpp                       # Instantiates NeuralNetworkGraph
├── graph.h                         # Graph definition with shared-buffer parameter staging
├── leaky_relu.cpp/.h               # Leaky ReLU activation kernel
├── bias_add.cpp/.h                 # Bias addition kernel
├── window_split_128_to_64x2.cpp/.h # Window splitter for cascade inputs
├── roll_concat.cpp/.h              # Roll-concat kernel (128→768)
├── window_split_768_to_*.h         # Additional window split utilities (unused)
├── aie.cfg                         # AIE compiler configuration
└── Makefile                        # Build targets
```

## Build & Simulation

### Compile AI Engine Graph
```bash
cd aieml7
make graph TARGET=hw       # or TARGET=hw_emu
```
Produces `Work/libadf.a` containing the compiled graph.

### Run AI Engine Simulation
```bash
make sim
```

## Data Flow

### Input/Output PLIO
- **Activation Input**: `layer0_in` reads from `../data/tmp_inp768.txt` (128 floats)
- **Weights Stream**: `weights_in` ingests the concatenated weights file `../data/solver_0_weights_stream.txt`
- **Bias Stream**: `biases_in` ingests the concatenated bias file `../data/solver_0_bias_stream.txt`
- **Output**: `layer_out` writes to `../data/subsolver_0_dense_3_output_aie.txt` (128 floats)

### Parameter Streaming
- All dense-layer weights are appended into a single stream file in the order dense0 → dense1 → dense2 → dense3.
- A matrix of shared buffer tiles (`weights_buffer`) holds every weight element; each cascade kernel reads its slice
  via `read_access` offsets.
- Bias vectors are concatenated into one bias stream and staged in `bias_buffer`, which exposes four window
  views (one per layer).
- The flow avoids packet headers and dedicated loader kernels—the PLIO stream writes directly into the shared buffers.

### Weight Buffer Sizing
- Total dense-layer weight elements: 147,456 floats = 589,824 bytes.
- AI Engine tile memory is 32 KiB, so a shared buffer cannot house the full concatenated weight stream.
- The shared-buffer grid arranges the aggregated stream into 18 tiles (one per cascade leg) capped at 8,192 floats each, keeping every tile comfortably under the 12,288-float shared-memory guidance while still presenting simple offset-based reads.

### Processing Pipeline
```
input(128) → roll_concat(128→768) → shared_buffer[768] →
dense0(768×128, casc=12) → bias → leaky_relu → split →
dense1(128×128, casc=2) → bias → leaky_relu → split →
dense2(128×128, casc=2) → bias → leaky_relu → split →
dense3(128×128, casc=2) → bias → leaky_relu → output(128)
```

## Shared Buffer Tiling

The roll-concat output uses a shared buffer with tiled access:
- **Buffer Size**: 768 floats
- **Number of Consumers**: 12 (cascade kernels in dense0)
- **Tile Size**: 64 floats per consumer
- **Write Access**: Single writer (roll_concat kernel) writes entire 768-element buffer
- **Read Access**: Each of 12 cascade kernels reads its own 64-element tile with stride offset

## Notes
- Test data is generated by `../data/generate_test_data.py`
- This architecture demonstrates efficient shared-buffer usage for large cascaded operations
- Kernel placement can be manually specified via `location<kernel>` directives in graph.h
- This graph serves as the "solver" component tested independently before integration into aieml9
