# AI Engine-ML Graph: Packet-Switched Two-Layer Neural Network

This directory implements the AI Engine graph that underpins the RTDA demo's
hidden-layer cascade. It showcases how ADF packet infrastructure can be used to
move activations between kernels while keeping the dense-layer compute blocks on
pure streaming interfaces.

## Architecture Overview

- **Input Layer**: 8-element float vector input via `input_data` PLIO
- **Dense 0 ("dense1")**: 8×128 matrix-vector multiplication using DSPLib (`dense8x128`)
- **Activation**: Leaky ReLU kernel (`k_lrelu0`) processes the 128-element hidden layer
- **Hidden Packetization**: `hidden_stream_to_packet_kernel` splits the activation
  stream into `CASCADE_LENGTH` packets (currently 4), with each packet containing
  32 elements (128/4 = 32 elements per cascade branch)
- **Cascade Fan-out**: `pktsplit<CASCADE_LENGTH>` distributes packets to cascade branches
- **Dense 1 ("dense2")**: 4 parallel 128×128 DSPLib matrix-vector kernels (CASCADE_LENGTH=4),
  each processing their portion of the hidden vector to produce final logits
- **Runtime Parameter Loading**: All dense kernels receive weights via RTP ports,
  enabling dynamic weight updates without graph recompilation

The packet-based hop between the Leaky ReLU and the cascade ensures each branch
receives only the portion of the hidden vector it needs while avoiding wide
fan-out stream connections.

## Directory Layout

```
├── graph.cpp                        # Instantiates `NeuralNetworkGraph` with RTP weight loading
├── graph.h                          # Graph definition with 4-way cascade packet workflow
├── kernels/
│   ├── stream_to_packet.cpp/h           # Converts input float stream to packets
│   ├── hidden_stream_to_packet.cpp/h    # Splits hidden activations into cascade packets
│   ├── packet_to_stream.cpp/h           # Converts packets back to streams for dense layers
│   ├── leaky_relu.cpp/h                 # Leaky ReLU activation kernel (slope=0.1)
│   └── roll_concat.cpp/h                # Produces 6 cyclic shifts of the dense output
├── split_stream.cpp/h               # Stream splitting utilities (deprecated)
├── aie.cfg                          # AI Engine compiler configuration
└── Makefile                         # Build rules with corrected DATA_DIR path
```

## Build

The supplied `Makefile` wraps the standard build flow using `v++` with AI Engine
configuration. From this directory, compile and simulate the graph with:

```bash
make graph TARGET=hw       # Compile AIE graph (default: hw)
make sim                   # Run AI Engine simulation with weight loading
make clean                 # Clean all build artifacts
```

**Note**: The Makefile has been updated to use absolute paths for `DATA_DIR` to fix
weight file resolution issues when `aiesimulator` runs from the `Work/temp/` directory.

To invoke the compiler directly:

```bash
v++ -c --mode aie --target hw graph.cpp \
    kernels/stream_to_packet.cpp kernels/hidden_stream_to_packet.cpp \
    kernels/packet_to_stream.cpp kernels/leaky_relu.cpp kernels/roll_concat.cpp \
    --platform=${PLATFORM} \
    --work_dir=Work \
    --config=aie.cfg \
    --include="./" \
    --include="../common" \
    --include="../dsp_lib/L1/src/aie" \
    --include="../dsp_lib/L1/include/aie" \
    --include="../dsp_lib/L2/include/aie"
```

Both commands produce `Work/libadf.a` inside this directory.

## Data Flow Walkthrough

1. **Input embedding**: `input_data` PLIO feeds an 8-element float vector into
   `stream_to_packet_kernel`, which tags the samples with an ADF packet header.
2. **Packet routing**: A single-input `pktsplit` forwards the dense0 packet to
   `packet_to_stream_kernel`, where it becomes a float stream and enters the
   first dense layer (`dense1`).
3. **Hidden activation**: The DSPLib dense kernel produces a 128-element vector
   which flows through `k_lrelu0` for Leaky ReLU activation.
4. **Hidden-layer packet hop**: `hidden_stream_to_packet_kernel` consumes the
   Leaky ReLU output, builds 4 packets (one per cascade lane), and marks TLAST
   on the final element.
5. **Cascade fan-out**: `pktsplit<4>` inspects each packet's ID and forwards it
   to the matching `packet_to_stream_hidden_kernel` instance, which converts the
   payload back into a float stream for the downstream dense kernel (`dense2.inB[i]`).
6. **Roll-and-concatenate**: The cascade of dense kernels feeds `roll_concat_kernel`,
   which emits six cyclically shifted copies (6 × 128 = 768 values) onto the
   `output_data` PLIO for downstream processing.

Throughout the flow the packets exist only on the inter-kernel hop where fan-out
is required, allowing the dense compute kernels to stay on standard stream
interfaces.

## Files and Build Instructions

The PLIO files consumed by the graph reside in the project-parent
[`../data/`](../../data) directory. They are generated by
[`data/generate_test_data.py`](../data/generate_test_data.py).
